{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain, combinations\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import threading\n",
    "from multiprocessing import Manager\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShardedEpisodicDataset():\n",
    "  def __init__(self, data_paths, meta_path, min_n_way, try_k_shot, try_n_query, split=\"train\", test_split=0.1, val_split=0.1, split_seed=33):\n",
    "    self.data_paths = data_paths\n",
    "    self.min_n_way = min_n_way\n",
    "    self.try_k_shot = try_k_shot\n",
    "    self.try_n_query = try_n_query\n",
    "\n",
    "    # load metadata\n",
    "    meta = torch.load(meta_path)\n",
    "    self.rel_idxs = meta['rel_idxs']\n",
    "    self.n_examples = self.rel_idxs[-1] + 1\n",
    "    meta_class_idxs = meta['class_idxs']\n",
    "    self.n_classes = len(meta_class_idxs)\n",
    "\n",
    "    # filter classes based on split type\n",
    "    all_idxs = list(range(self.n_examples))\n",
    "    train_idxs, test_idxs = train_test_split(all_idxs, test_size=test_split + val_split, random_state=split_seed)\n",
    "    test_idxs, val_idxs = train_test_split(test_idxs, test_size=val_split / (test_split + val_split), random_state=split_seed)\n",
    "    print(f\"train_size : {len(train_idxs)}, test_size : {len(test_idxs)}, val_size : {len(val_idxs)}\")\n",
    "    if split == \"train\":\n",
    "      valid_idxs = set(train_idxs)\n",
    "    elif split == \"test\":\n",
    "      valid_idxs = set(test_idxs)\n",
    "    elif split == 'val':\n",
    "      valid_idxs = set(val_idxs)\n",
    "    else:\n",
    "      raise ValueError(\"Critical: split is not train, test or val\")\n",
    "\n",
    "    self.class_idxs = [[i.item() for i in meta_class_idxs[c] if i.item() in valid_idxs] for c in range(self.n_classes)]\n",
    "\n",
    "    # filter valid classes\n",
    "    self.valid_classes = [\n",
    "      c for c, indices in enumerate(self.class_idxs)\n",
    "      if len(indices) >= self.try_k_shot + self.try_n_query\n",
    "    ]\n",
    "\n",
    "    print(f\"{len(self.valid_classes)} of {len(self.class_idxs)} classes have atleast try_k_shot + try_n_query ({self.try_k_shot + self.try_n_query}) examples\")\n",
    "\n",
    "    if len(self.valid_classes) < self.min_n_way:\n",
    "      raise ValueError(f\"critical: try_k_shot + try_n_query is too big or min_n_way is too big, there are {len(self.valid_classes)} valid classes\")\n",
    "\n",
    "    # cache management\n",
    "    self.cache_size = 1\n",
    "    self.manager = Manager()\n",
    "    self.cache = self.manager.dict()\n",
    "    self.lock = threading.Lock()\n",
    "\n",
    "  def __len__(self):\n",
    "    return 1 # cuz this number sounds good\n",
    "\n",
    "  def _get_idxs(self, idx):\n",
    "    # binary search on shard to find shard\n",
    "    low, high = 0, len(self.rel_idxs) - 1\n",
    "    while (low < high):\n",
    "      mid = (low + high) // 2\n",
    "      if self.rel_idxs[mid] < idx:\n",
    "        low = mid + 1\n",
    "      else:\n",
    "        high = mid\n",
    "    shard_idx = low\n",
    "    \n",
    "    if (shard_idx == 0):\n",
    "      abs_idx = idx\n",
    "    else:\n",
    "      abs_idx = idx - self.rel_idxs[shard_idx - 1] - 1\n",
    "    return shard_idx, abs_idx\n",
    "\n",
    "  def _load_shard(self, shard_idx):\n",
    "    shard_path = self.data_paths[shard_idx]\n",
    "    \n",
    "    with self.lock:\n",
    "      # Check if shard is already in cache\n",
    "      if shard_path in self.cache:\n",
    "        return self.cache[shard_path]\n",
    "            \n",
    "      # If cache is full, remove one item\n",
    "      if len(self.cache) >= self.cache_size:\n",
    "        # More predictable eviction strategy\n",
    "        oldest_key = next(iter(self.cache))\n",
    "        self.cache.pop(oldest_key)\n",
    "            \n",
    "      # Load the shard and store in cache\n",
    "      shard = torch.load(shard_path)\n",
    "      self.cache[shard_path] = shard\n",
    "        \n",
    "    return shard\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Choose random subset of n_way classes\n",
    "    selected_classes = random.sample(self.valid_classes, self.min_n_way)\n",
    "\n",
    "    support_indices, query_indices = [], []\n",
    "\n",
    "    for c in selected_classes:\n",
    "      indices = self.class_idxs[c]\n",
    "      if not indices:\n",
    "        continue\n",
    "      selected_indices = random.sample(indices, self.try_k_shot + self.try_n_query)\n",
    "      support_indices.extend(selected_indices[:self.try_k_shot])\n",
    "      query_indices.extend(selected_indices[self.try_k_shot:])\n",
    "\n",
    "    support_indices = torch.tensor(support_indices, dtype=torch.long)\n",
    "    query_indices = torch.tensor(query_indices, dtype=torch.long)\n",
    "\n",
    "    unique_shards = set(self._get_idxs(i)[0] for i in support_indices.tolist() + query_indices.tolist())\n",
    "    shard_data = {shard_idx: self._load_shard(shard_idx) for shard_idx in unique_shards}\n",
    "\n",
    "    # Fetch data/labels from those shards\n",
    "    support_data = torch.cat([shard_data[self._get_idxs(i)[0]]['imgs'][self._get_idxs(i)[1]].unsqueeze(0) for i in support_indices])\n",
    "    support_label = torch.cat([shard_data[self._get_idxs(i)[0]]['lbls'][self._get_idxs(i)[1]].unsqueeze(0) for i in support_indices])\n",
    "    query_data = torch.cat([shard_data[self._get_idxs(i)[0]]['imgs'][self._get_idxs(i)[1]].unsqueeze(0) for i in query_indices])\n",
    "    query_label = torch.cat([shard_data[self._get_idxs(i)[0]]['lbls'][self._get_idxs(i)[1]].unsqueeze(0) for i in query_indices])\n",
    "\n",
    "    return {\n",
    "      'support_data': support_data, # (~n_way * ~k_shot, C, H, W)\n",
    "      'support_label': support_label, # (~n_way * ~k_shot, num_classes)\n",
    "      'query_data': query_data, # (~n_way * ~n_query, C, H, W)\n",
    "      'query_label': query_label # (~n_way * ~n_query, num_classes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_6436\\384058308.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta = torch.load(meta_path)\n"
     ]
    }
   ],
   "source": [
    "data_paths = list(Path('./processed').rglob('*.pth'))\n",
    "data_paths.sort()\n",
    "meta_path = data_paths.pop()\n",
    "meta = torch.load(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size : 800, test_size : 100, val_size : 100\n",
      "88 of 200 classes have atleast try_k_shot + try_n_query (4) examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_6436\\333209757.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta = torch.load(meta_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size : 800, test_size : 100, val_size : 100\n",
      "16 of 200 classes have atleast try_k_shot + try_n_query (4) examples\n",
      "train_size : 800, test_size : 100, val_size : 100\n",
      "12 of 200 classes have atleast try_k_shot + try_n_query (4) examples\n"
     ]
    }
   ],
   "source": [
    "dataset = ShardedEpisodicDataset(data_paths, meta_path, 2, 2, 2, split=\"train\")\n",
    "val_dataset = ShardedEpisodicDataset(data_paths, meta_path, 2, 2, 2, split=\"val\")\n",
    "test_dataset = ShardedEpisodicDataset(data_paths, meta_path, 2, 2, 2, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(model, lambda_l2=1e-4):\n",
    "    l2_norm = 0\n",
    "    for param in model.parameters():\n",
    "        l2_norm += param.pow(2).sum()\n",
    "    return lambda_l2 * l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    resnet = models.resnet18(weights=True)\n",
    "    resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "    self.encoder = resnet\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x) # (batch_size, 512, 7, 7) for input of (batch_size, 3, 224, 224)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProtoNet stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "  n_class, n_sample, n_feature = y.shape[0], x.shape[0], x.shape[1]\n",
    "\n",
    "  x = x.unsqueeze(1).expand(-1, n_class, -1)\n",
    "  y = y.unsqueeze(0).expand(n_sample, -1, -1)\n",
    "\n",
    "  return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "  def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "    super().__init__()\n",
    "    self.gamma = gamma\n",
    "    self.alpha = alpha\n",
    "    self.reduction = reduction\n",
    "\n",
    "  def forward(self, logits, targets):\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-bce_loss)\n",
    "    focal_loss = ((1 - pt) ** self.gamma) * bce_loss\n",
    "    if self.alpha is not None:\n",
    "      focal_loss = self.alpha * focal_loss\n",
    "    if self.reduction == 'mean':\n",
    "      return focal_loss.mean()\n",
    "    elif self.reduction == 'sum':\n",
    "      return focal_loss.sum()\n",
    "    else:\n",
    "      return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mAP(y_true, y_scores):\n",
    "  num_classes = y_true.shape[1]\n",
    "  APs = []\n",
    "  for i in range(num_classes):\n",
    "    if np.sum(y_true[:, i]) >0:\n",
    "      AP = average_precision_score(y_true[:, i], y_scores[:, i])\n",
    "      APs.append(AP)\n",
    "  return np.mean(APs) if len(APs) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototypical_loss(model, episode, device, use_focal=True, gamma=2.0, alpha=1.0):\n",
    "  model.train()\n",
    "  model.to(device)\n",
    "  support_label = episode['support_label'].to(device)\n",
    "  query_label = episode['query_label'].to(device)\n",
    "  support_data = episode['support_data'].to(device)\n",
    "  query_data = episode['query_data'].to(device)\n",
    "\n",
    "  # create extended vector of labels for each support and query\n",
    "  num_classes = episode['support_label'].shape[1]\n",
    "\n",
    "  unique_combinations = set()\n",
    "  for label_set in support_label:\n",
    "      active = tuple(i for i, val in enumerate(label_set) if val == 1)\n",
    "      if active:\n",
    "          unique_combinations.add(active)\n",
    "  for label_set in query_label:\n",
    "      active = tuple(i for i, val in enumerate(label_set) if val == 1)\n",
    "      if active:\n",
    "          unique_combinations.add(active)\n",
    "\n",
    "  def get_power_set(indices):\n",
    "    s = list(indices)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(1, len(s) + 1)))\n",
    "\n",
    "  all_subsets = set()\n",
    "  for idx_comb in unique_combinations:\n",
    "    for subsets in get_power_set(idx_comb):\n",
    "      all_subsets.add(tuple(sorted(subsets)))\n",
    "\n",
    "  all_subsets = sorted(all_subsets, key=lambda s: (len(s), s))\n",
    "  subset_map = {subset: i for i, subset in enumerate(all_subsets)} # subset tuple to idx in extended multi hot encoded\n",
    "\n",
    "  def extend_labels(labels):\n",
    "    ext = torch.zeros((len(labels), len(all_subsets)), dtype=torch.float, device=device)\n",
    "    for i, row in enumerate(labels):\n",
    "      active_indices = tuple(sorted(j for j, val in enumerate(row) if val == 1))\n",
    "      if active_indices:\n",
    "        for subset in get_power_set(active_indices):\n",
    "          subset = tuple(sorted(subset))\n",
    "          ext[i, subset_map[subset]] = 1\n",
    "    return ext\n",
    "\n",
    "  support_ext_lbls = extend_labels(support_label)\n",
    "  query_ext_lbls = extend_labels(query_label)\n",
    "\n",
    "  # calculate prototype for each label combination\n",
    "  \"\"\"\n",
    "  support_embeddings = []\n",
    "  for x in support_data:\n",
    "    x = x.unsqueeze(0)\n",
    "    embed = model(x)\n",
    "    support_embeddings.append(embed.cpu())\n",
    "    del x, embed  # Manually delete tensors\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "  support_embeddings = torch.cat(support_embeddings, dim=0)  # Stack results\n",
    "\n",
    "  query_embeddings = []\n",
    "  for x in query_data:\n",
    "    x = x.unsqueeze(0)\n",
    "    embed = model(x)\n",
    "    query_embeddings.append(embed.cpu())\n",
    "    del x, embed  # Manually delete tensors\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "  query_embeddings = torch.cat(query_embeddings, dim=0)  # Stack results\n",
    "  \"\"\"\n",
    "  support_embeddings = model(support_data)\n",
    "  query_embeddings = model(query_data)\n",
    "\n",
    "  d = support_embeddings.shape[1]\n",
    "  num_proto = len(all_subsets)\n",
    "  prototypes = torch.zeros((num_proto, d), device=device)\n",
    "  counts = torch.zeros(num_proto, device=device)\n",
    "\n",
    "  for i in range(support_embeddings.shape[0]):\n",
    "      active_prototypes = support_ext_lbls[i].nonzero(as_tuple=True)[0]\n",
    "      for p in active_prototypes:\n",
    "          prototypes[p] += support_embeddings[i]\n",
    "          counts[p] += 1\n",
    "  counts = counts.clamp(min=1)\n",
    "  prototypes /= counts.unsqueeze(1)  # Compute mean embeddings\n",
    "\n",
    "  # Compute distances between query embeddings and prototypes\n",
    "  distances = euclidean_dist(query_embeddings, prototypes)  # Shape: (num_query, num_subsets)\n",
    "  logits = -distances\n",
    "\n",
    "  if use_focal:\n",
    "    pos_freq = query_ext_lbls.float().mean(dim=0)\n",
    "    epsilon = 1e-6\n",
    "    raw_pos_weight = 1.0 / (pos_freq + epsilon)\n",
    "    alpha = torch.clamp(raw_pos_weight, max=10.0)\n",
    "    loss_fn = FocalLoss(gamma=gamma, alpha=torch.tensor(alpha, device=device) if isinstance(alpha, (list, np.ndarray)) else alpha)\n",
    "  else:\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "  loss = loss_fn(logits, query_ext_lbls)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    probs = torch.sigmoid(logits)\n",
    "    y_true = query_ext_lbls.cpu().numpy()\n",
    "    y_scores = probs.cpu().numpy()\n",
    "    mAP = compute_mAP(y_true, y_scores)\n",
    "\n",
    "  pred = (probs > 0.5).float()\n",
    "  true_bool = query_ext_lbls.bool()\n",
    "  tp = (pred * true_bool.float()).sum().float()\n",
    "  fp = (pred * (~true_bool).float()).sum().float()\n",
    "  fn = ((~pred.bool()) * true_bool.float()).sum().float()\n",
    "  precision = tp / (tp + fp + 1e-6)\n",
    "  recall = tp / (tp + fn + 1e-6)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "  return loss, f1_score, mAP\n",
    "\n",
    "  # # Apply sigmoid and compute loss using BCE\n",
    "  # #print(distances.shape)\n",
    "  # #print(query_ext_lbls.float().shape)\n",
    "  # pos_freq = query_ext_lbls.float().mean(dim=0)\n",
    "  # epsilon = 1e-6\n",
    "  # raw_pos_weight = 1.0 / (pos_freq + epsilon)\n",
    "  # max_weight = 10.0\n",
    "  # pos_weight  = torch.clamp(raw_pos_weight, max = max_weight)\n",
    "  # loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
    "  # loss = loss_fn(distances, query_ext_lbls.float())\n",
    "\n",
    "  # # compress the extended prediction back to normal\n",
    "  # pred = torch.zeros(episode['query_data'].shape[0], num_classes, dtype=torch.bool, device=device)\n",
    "  # for i in range(episode['query_data'].shape[0]):\n",
    "  #   min_dist = torch.min(distances[i])  # 1. Get smallest distance value for this query\n",
    "  #   candidates = (distances[i] == min_dist).nonzero(as_tuple=True)[0]  # 2. Find ALL prototypes with this distance\n",
    "  #   best_idx = max(candidates, key=lambda idx: len(all_subsets[idx]))  # 3. Select largest subset\n",
    "  #   pred[i, all_subsets[best_idx]] = True\n",
    "\n",
    "  # true_bool = episode['query_label'].to(device).bool()\n",
    "  # tp = (pred & true_bool).sum().float()\n",
    "  # fp = (pred & ~true_bool).sum().float()\n",
    "  # fn = (~pred & true_bool).sum().float()\n",
    "\n",
    "  # precision = tp / (tp + fp + 1e-6)  # Adding small value to avoid division by zero\n",
    "  # recall = tp / (tp + fn + 1e-6)\n",
    "  # f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "  # return loss, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, episode, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    num_classes = episode['support_label'].shape[1]\n",
    "    unique_combinations = set()\n",
    "    for label_set in episode['support_label']:\n",
    "        active = tuple(i for i, val in enumerate(label_set) if val == 1)\n",
    "        if active:\n",
    "            unique_combinations.add(active)\n",
    "    for label_set in episode['query_label']:\n",
    "        active = tuple(i for i, val in enumerate(label_set) if val == 1)\n",
    "        if active:\n",
    "            unique_combinations.add(active)\n",
    "\n",
    "    def get_power_set(indices):\n",
    "        return list(chain.from_iterable(combinations(indices, r) for r in range(1, len(indices) + 1)))\n",
    "\n",
    "    all_subsets = set()\n",
    "    for idx_comb in unique_combinations:\n",
    "        for subset in get_power_set(idx_comb):\n",
    "            all_subsets.add(tuple(sorted(subset)))\n",
    "    all_subsets = sorted(all_subsets, key=lambda s: (len(s), s))\n",
    "    subset_map = {subset: i for i, subset in enumerate(all_subsets)}\n",
    "\n",
    "    def extend_labels(labels):\n",
    "        ext = torch.zeros((len(labels), len(all_subsets)), dtype=torch.float, device=device)\n",
    "        for i, row in enumerate(labels):\n",
    "            active_indices = tuple(sorted(j for j, val in enumerate(row) if val == 1))\n",
    "            if active_indices:\n",
    "                for subset in get_power_set(active_indices):\n",
    "                    subset = tuple(sorted(subset))\n",
    "                    ext[i, subset_map[subset]] = 1.0\n",
    "        return ext\n",
    "\n",
    "    support_ext_lbls = extend_labels(episode['support_label'])\n",
    "    query_ext_lbls = extend_labels(episode['query_label'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        support_embeddings = model(episode['support_data'].to(device))\n",
    "        query_embeddings = model(episode['query_data'].to(device))\n",
    "    d = support_embeddings.shape[1]\n",
    "    num_proto = len(all_subsets)\n",
    "    prototypes = torch.zeros((num_proto, d), device=device)\n",
    "    counts = torch.zeros(num_proto, device=device)\n",
    "    for i in range(support_embeddings.shape[0]):\n",
    "        active_prototypes = support_ext_lbls[i].nonzero(as_tuple=True)[0]\n",
    "        for p in active_prototypes:\n",
    "            prototypes[p] += support_embeddings[i]\n",
    "            counts[p] += 1\n",
    "    counts = counts.clamp(min=1)\n",
    "    prototypes /= counts.unsqueeze(1)\n",
    "\n",
    "    distances = euclidean_dist(query_embeddings, prototypes)\n",
    "    logits = -distances  # Convert distances to logits\n",
    "    probs = torch.sigmoid(logits)\n",
    "    pred = (probs > 0.5).int()\n",
    "\n",
    "    return pred, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.3, top_k=5):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        self.top_k = top_k\n",
    "\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"c_out must be divisible by num_heads\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out))\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.projection.weight, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a, gain=1.414)\n",
    "\n",
    "    def compute_adj_matrix(self, node_feats, drop_prob=0.2):\n",
    "        # Normalize features along the last dimension (using lowercase 'normalize')\n",
    "        norm_feats = F.normalize(node_feats, p=2, dim=-1)\n",
    "        # Here, node_feats has shape (batch_size, num_nodes, feature_dim).\n",
    "        # We need to compute pairwise similarity per batch.\n",
    "        # Use torch.bmm to perform batched matrix multiplication.\n",
    "        sim = torch.bmm(norm_feats, norm_feats.transpose(1, 2))  # (batch, num_nodes, num_nodes)\n",
    "        # Create a binary adjacency matrix based on a threshold (e.g., 0.5)\n",
    "        if self.top_k is not None:\n",
    "          B, N, _ = sim.shape\n",
    "          topk_values, top_k_indices = torch.topk(sim, k=self.top_k, dim=-1)\n",
    "          mask = torch.zeros_like(sim)\n",
    "          mask.scatter_(2, top_k_indices, 1.0)\n",
    "          sim = sim * mask\n",
    "\n",
    "        adj_matrix = (sim > 0.4).float()\n",
    "\n",
    "        if drop_prob > 0.0:\n",
    "          drop_mask = (torch.rand_like(adj_matrix) > drop_prob).float()\n",
    "          adj_matrix = adj_matrix * drop_mask\n",
    "\n",
    "        return adj_matrix\n",
    "\n",
    "    def forward(self, node_feats, print_attn_probs=False):\n",
    "        batch_size, num_nodes, _ = node_feats.shape\n",
    "\n",
    "        # Compute the adjacency matrix from node features.\n",
    "        adj_matrix = self.compute_adj_matrix(node_feats, drop_prob=0.2)  # Expected shape: (batch_size, num_nodes, num_nodes)\n",
    "\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # Now, get the indices of nonzero elements in the adj matrix.\n",
    "        # Note: For a batched adj_matrix, nonzero() returns indices with shape (num_edges, 3)\n",
    "        edges = (adj_matrix > 0.0).nonzero(as_tuple=False)\n",
    "        # edges[:, 0] is the batch index, edges[:, 1] is the row index, and edges[:, 2] is the column index.\n",
    "        batch_indices = edges[:, 0]\n",
    "        offset = batch_indices * num_nodes\n",
    "        edges_indices_row = offset + edges[:, 1]\n",
    "        edges_indices_col = offset + edges[:, 2]\n",
    "\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(node_feats_flat, dim=0, index=edges_indices_row),\n",
    "            torch.index_select(node_feats_flat, dim=0, index=edges_indices_col)\n",
    "        ], dim=-1)\n",
    "\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Create an attention matrix with shape (batch_size, num_nodes, num_nodes, num_heads)\n",
    "        attn_matrix = torch.full((*adj_matrix.shape, self.num_heads), -1e9, device=node_feats.device)\n",
    "\n",
    "        # Assign computed logits to positions where adj_matrix is 1.\n",
    "        # We expand adj_matrix to have a head dimension.\n",
    "        attn_matrix[(adj_matrix > 0.0).unsqueeze(-1).expand(-1, -1, -1, self.num_heads)] = attn_logits.reshape(-1)\n",
    "\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "\n",
    "        if print_attn_probs:\n",
    "            print(\"attention probs \\n\", attn_probs.permute(0, 3, 1, 2).detach().cpu())\n",
    "\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnGat(nn.Module):\n",
    "  def __init__(self, cnn_encoder, gat_layer, embed_size, device, cnn_embed_size=512):\n",
    "    super().__init__()\n",
    "    self.device = device\n",
    "    self.embed_size = embed_size\n",
    "    self.cnn_encoder = cnn_encoder()\n",
    "    self.gat_layer = gat_layer(\n",
    "      c_in=cnn_embed_size,\n",
    "      c_out=self.embed_size // 49, # there are 7x7 nodes from the cnn, so we calc how many features each node can have to have embed_size size when flattened\n",
    "    )\n",
    "    self.cnn_encoder.to(device)\n",
    "    self.gat_layer.to(device)\n",
    "\n",
    "  def forward(self, data):\n",
    "    cnn_embeds = self.cnn_encoder(data) # (len data, cnn_embed_size, H, W) h, w = 7, 7\n",
    "    B, C, H, W = cnn_embeds.shape\n",
    "    nodes = cnn_embeds.permute(0, 2, 3, 1).reshape(B, H*W, C)\n",
    "    nodes = self.gat_layer(nodes) # (len data, h * w, embedsize / 49)\n",
    "    gat_embeds = nodes.view(nodes.size(0), -1) # (len data, embed size)\n",
    "    return gat_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "def check_val(model, num_checks=10):\n",
    "    running_f1 = 0.0\n",
    "    min_f1 = float('inf')\n",
    "    max_f1 = float('-inf')\n",
    "    running_map = 0.0\n",
    "    for i in range(num_checks):\n",
    "        test_ep = val_dataset[0]\n",
    "        _, f1, mAP = prototypical_loss(model, test_ep, device, use_focal=True, gamma=2.0, alpha=1.0)\n",
    "        f1_val = f1.item() if torch.is_tensor(f1) else f1\n",
    "        running_f1 += f1_val\n",
    "        running_map += mAP\n",
    "        min_f1 = min(min_f1, f1_val)\n",
    "        max_f1 = max(max_f1, f1_val)\n",
    "\n",
    "    avg_f1 = running_f1 / num_checks\n",
    "    avg_map = running_map / num_checks\n",
    "    print(f\"Val: Avg F1: {avg_f1:.2f}, min: {min_f1:.2f}, max: {max_f1:.2f}, Avg mAP: {avg_map:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, batch_size, epochs, device):\n",
    "  model.train()\n",
    "  model.to(device)\n",
    "\n",
    "  random.seed(42)\n",
    "  torch.manual_seed(42)\n",
    "\n",
    "  scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_f1 = 0.0\n",
    "    for batch in tqdm(range(batch_size)):\n",
    "      optimizer.zero_grad()\n",
    "      episode = dataset[0]\n",
    "\n",
    "      loss, f1, mAP = prototypical_loss(model, episode, device, use_focal=True, gamma=2.0, alpha=1.0)\n",
    "      loss += l2_regularization(model, lambda_l2=1e-4)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "      running_f1 += f1.item()\n",
    "\n",
    "    epoch_loss = running_loss / batch_size\n",
    "    epoch_f1 = running_f1 / batch_size\n",
    "    epoch_mAP = mAP\n",
    "    print(f'Epoch {epoch+1} Complete - Loss: {epoch_loss:.4f} F1: {epoch_f1:.2f} mAP: {epoch_mAP:.2f}')\n",
    "    check_val(model)\n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\.conda\\envs\\pytorch-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331aaefa033a4b4ba532edaa162457fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "Epoch 1 Complete - Loss: 255.6853 F1: 0.00 mAP: 0.30\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "Val: Avg F1: 0.00, min: 0.00, max: 0.00, Avg mAP: 0.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775e7270d0fe4944a8914c29070e591f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "Epoch 2 Complete - Loss: 569.6907 F1: 0.00 mAP: 0.30\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "Val: Avg F1: 0.00, min: 0.00, max: 0.00, Avg mAP: 0.27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4b7877d8e94a428020669aaedb06ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "Epoch 3 Complete - Loss: 1205.1200 F1: 0.00 mAP: 0.43\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "support data size torch.Size([4, 3, 224, 224]), query data size torch.Size([4, 3, 224, 224])\n",
      "Val: Avg F1: 0.00, min: 0.00, max: 0.00, Avg mAP: 0.30\n"
     ]
    }
   ],
   "source": [
    "model = CnnGat(ConvEncoder, GATLayer, embed_size=245, device=device)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, weight_decay=1e-4)\n",
    "\n",
    "train(model, dataset, optimizer, batch_size=1, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = test_dataset[0]\n",
    "\n",
    "# print shape for debugging\n",
    "print(f\"{test_episode['support_data'].shape}, {test_episode['support_label'].shape}, {test_episode['query_data'].shape}, {test_episode['query_label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction and performance\n",
    "pred, f1 = predict(model, test_episode, device)\n",
    "print(f\"f1 of test episode {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for visualization\n",
    "imgs = test_episode['query_data'].cpu().numpy().transpose(0, 2, 3, 1)\n",
    "col_names = ['Accelerating_and_revving_and_vroom', 'Accordion', 'Acoustic_guitar', 'Aircraft', 'Alarm', 'Animal', 'Applause', 'Bark', 'Bass_drum', 'Bass_guitar', 'Bathtub_(filling_or_washing)', 'Bell', 'Bicycle', 'Bicycle_bell', 'Bird', 'Bird_vocalization_and_bird_call_and_bird_song', 'Boat_and_Water_vehicle', 'Boiling', 'Boom', 'Bowed_string_instrument', 'Brass_instrument', 'Breathing', 'Burping_and_eructation', 'Bus', 'Buzz', 'Camera', 'Car', 'Car_passing_by', 'Cat', 'Chatter', 'Cheering', 'Chewing_and_mastication', 'Chicken_and_rooster', 'Child_speech_and_kid_speaking', 'Chime', 'Chink_and_clink', 'Chirp_and_tweet', 'Chuckle_and_chortle', 'Church_bell', 'Clapping', 'Clock', 'Coin_(dropping)', 'Computer_keyboard', 'Conversation', 'Cough', 'Cowbell', 'Crack', 'Crackle', 'Crash_cymbal', 'Cricket', 'Crow', 'Crowd', 'Crumpling_and_crinkling', 'Crushing', 'Crying_and_sobbing', 'Cupboard_open_or_close', 'Cutlery_and_silverware', 'Cymbal', 'Dishes_and_pots_and_pans', 'Dog', 'Domestic_animals_and_pets', 'Domestic_sounds_and_home_sounds', 'Door', 'Doorbell', 'Drawer_open_or_close', 'Drill', 'Drip', 'Drum', 'Drum_kit', 'Electric_guitar', 'Engine', 'Engine_starting', 'Explosion', 'Fart', 'Female_singing', 'Female_speech_and_woman_speaking', 'Fill_(with_liquid)', 'Finger_snapping', 'Fire', 'Fireworks', 'Fixed-wing_aircraft_and_airplane', 'Fowl', 'Frog', 'Frying_(food)', 'Gasp', 'Giggle', 'Glass', 'Glockenspiel', 'Gong', 'Growling', 'Guitar', 'Gull_and_seagull', 'Gunshot_and_gunfire', 'Gurgling', 'Hammer', 'Hands', 'Harmonica', 'Harp', 'Hi-hat', 'Hiss', 'Human_group_actions', 'Human_voice', 'Idling', 'Insect', 'Keyboard_(musical)', 'Keys_jangling', 'Knock', 'Laughter', 'Liquid', 'Livestock_and_farm_animals_and_working_animals', 'Male_singing', 'Male_speech_and_man_speaking', 'Mallet_percussion', 'Marimba_and_xylophone', 'Mechanical_fan', 'Mechanisms', 'Meow', 'Microwave_oven', 'Motor_vehicle_(road)', 'Motorcycle', 'Music', 'Musical_instrument', 'Ocean', 'Organ', 'Packing_tape_and_duct_tape', 'Percussion', 'Piano', 'Plucked_string_instrument', 'Pour', 'Power_tool', 'Printer', 'Purr', 'Race_car_and_auto_racing', 'Rail_transport', 'Rain', 'Raindrop', 'Ratchet_and_pawl', 'Rattle', 'Rattle_(instrument)', 'Respiratory_sounds', 'Ringtone', 'Run', 'Sawing', 'Scissors', 'Scratching_(performance_technique)', 'Screaming', 'Screech', 'Shatter', 'Shout', 'Sigh', 'Singing', 'Sink_(filling_or_washing)', 'Siren', 'Skateboard', 'Slam', 'Sliding_door', 'Snare_drum', 'Sneeze', 'Speech', 'Speech_synthesizer', 'Splash_and_splatter', 'Squeak', 'Stream', 'Strum', 'Subway_and_metro_and_underground', 'Tabla', 'Tambourine', 'Tap', 'Tearing', 'Telephone', 'Thump_and_thud', 'Thunder', 'Thunderstorm', 'Tick', 'Tick-tock', 'Toilet_flush', 'Tools', 'Traffic_noise_and_roadway_noise', 'Train', 'Trickle_and_dribble', 'Truck', 'Trumpet', 'Typewriter', 'Typing', 'Vehicle', 'Vehicle_horn_and_car_horn_and_honking', 'Walk_and_footsteps', 'Water', 'Water_tap_and_faucet', 'Waves_and_surf', 'Whispering', 'Whoosh_and_swoosh_and_swish', 'Wild_animals', 'Wind', 'Wind_chime', 'Wind_instrument_and_woodwind_instrument', 'Wood', 'Writing', 'Yell', 'Zipper_(clothing)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare grid\n",
    "grid_size = int(np.ceil(np.sqrt(imgs.shape[0])))\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(imgs.shape[0]):\n",
    "    ax = axes[i]\n",
    "    img = imgs[i]\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    active_indices = torch.nonzero(pred[i], as_tuple=True)[0]\n",
    "    selected_labels = [col_names[i] for i in active_indices]\n",
    "\n",
    "    # get query labels\n",
    "    query_labels = torch.nonzero(test_episode['query_label'][i], as_tuple=True)[0]\n",
    "    query_label_names = [col_names[i] for i in query_labels]\n",
    "\n",
    "    ax.set_title(f\"Pred: {', '.join(selected_labels)}\\nTrue: {', '.join(query_label_names)}\", fontsize=7)\n",
    "\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get avg f1 in test set\n",
    "\n",
    "checks = 200\n",
    "running_f1 = 0.0\n",
    "min_f1 = 2.0\n",
    "max_f1 = -1.0\n",
    "for i in tqdm(range(checks)):\n",
    "  test_ep = test_dataset.getEpisode(6, 6, 6)\n",
    "  _, f1 = predict(model, test_ep, device)\n",
    "  min_f1 = min(min_f1, f1)\n",
    "  max_f1 = max(max_f1, f1)\n",
    "  running_f1 += f1\n",
    "\n",
    "running_f1 = running_f1 / checks\n",
    "print(f\"Avg f1 in test set {running_f1:.2f}, min: {min_f1:.2f}, max: {max_f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
